{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6feed522",
   "metadata": {},
   "source": [
    "# LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b95384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import splev, splrep\n",
    "import scipy.io as sio\n",
    "from scipy.signal import resample\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96dde8",
   "metadata": {},
   "source": [
    "# CARGA DEL ARCHIVO CON LOS SEGMENTOS FILTRADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.load('filtrado_70mv.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos comprobaciones\n",
    "\n",
    "total_length = 0\n",
    "\n",
    "# Iterar sobre cada archivo en el .npz\n",
    "for key in loaded_data.files:\n",
    "    signal_dict = loaded_data[key].item()\n",
    "    dict_length = len(signal_dict)\n",
    "    \n",
    "    \n",
    "    # Sumar la longitud del diccionario actual al total acumulado\n",
    "    total_length += dict_length\n",
    "\n",
    "# Imprimir la suma total de las longitudes de los diccionarios\n",
    "print(\"Total length:\", total_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c498217",
   "metadata": {},
   "source": [
    "# MÉTODOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4586e4f",
   "metadata": {},
   "source": [
    "## MÉTODO ESPECTRAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c2187",
   "metadata": {},
   "source": [
    "### FUNCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd9bf0",
   "metadata": {},
   "source": [
    "A) FUNCIÓN MÉTODO ESPECTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SM_TWA(M_Wind):\n",
    "\"\"\"\n",
    "Esta función aplica el método espectral mejorado (SM) explicado en:\n",
    "\n",
    "MG Fernández-Calvillo et al., \"Machine Learning approach for TWA detection relying on ensemble data design,\" Heliyon, Vol. 9, no. 1, enero 2023.\n",
    "\n",
    "El método puede trabajar tanto con complejos ST-T como con complejos ST-T diferenciales. El SM estima la densidad espectral de potencia de cada serie de latidos\n",
    "Sj mediante el periodograma:\n",
    "\n",
    "             Pj=(1/M)|FFT({Sj})|^2\n",
    "\n",
    "donde M representa el número de latidos tomados para aplicar el SM. El espectro de potencia promedio de todas las series de latidos es:\n",
    "\n",
    "                 P=(1/N)∑Pj\n",
    "\n",
    "Parámetros:\n",
    "    M_Wind (array): Array que contiene la señal correspondiente a una ventana.\n",
    "\n",
    "Retornos:\n",
    "    AverPSD (array): Espectro promedio.\n",
    "    K_score (float): Valor de TWAR.\n",
    "    Valt (float): Onda alternante estimada.\n",
    "\"\"\"\n",
    "\n",
    "    M = len(M_Wind) \n",
    "    N = len(M_Wind[0]) \n",
    "    L = int(2 ** np.ceil(np.log2(M)))  # Resolución de la FFT.\n",
    "  \n",
    "\n",
    "\n",
    "    PSD_BeatSeries = np.abs(np.fft.fft(M_Wind, L, axis=0)) ** 2\n",
    "    AverPSD = np.mean(PSD_BeatSeries, axis=1) / (M * N)\n",
    "\n",
    "    noise_band = AverPSD[int(0.66 * ((L / 2)-1)):int(0.96 * ((L / 2)-1))]\n",
    "\n",
    "\n",
    "    noise_mean = np.mean(noise_band)\n",
    "\n",
    "    noise_std = np.std(noise_band)\n",
    "\n",
    "    T = AverPSD[1 + int(((L / 2)-1))]\n",
    "\n",
    "    K_score = (T - noise_mean) / noise_std\n",
    "\n",
    "    if K_score > 0:\n",
    "        Valt = np.sqrt(T - noise_mean) / np.sqrt(M)   # La división por sqrt(M) sirve para compensar un factor de ponderación.\n",
    "    else:\n",
    "        Valt = 0\n",
    "\n",
    "\n",
    "\n",
    "    return AverPSD, K_score, Valt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5c01c",
   "metadata": {},
   "source": [
    "B) FUNCIÓN DE CLASIFICIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e95557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(K_score):\n",
    "    labels = [] # Lista para almacenar los 0 o 1\n",
    "\n",
    "    for i in K_score:\n",
    "        if i < 3:  # el umbral se establece en 3 \n",
    "            labels.append(0) # no posee alternancia se agrega a la lista como 0 porque el kscore es menor que 3\n",
    "        else:\n",
    "            labels.append(1) # posee alternancia, se agrega a la lista como 1 porque el kscore es mayor que 3\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b5ca8",
   "metadata": {},
   "source": [
    "C) FUNCION DE EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(true_labels, predicted_labels):\n",
    "    confusion_mat = confusion_matrix(true_labels, predicted_labels) # MAtriz de confusión\n",
    "    precision = precision_score(true_labels, predicted_labels) # Cálculo precisión\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels) # Cálculo exactitud\n",
    "    recall = recall_score(true_labels, predicted_labels) # Cálculo sensibilidad\n",
    "    f1 = f1_score(true_labels, predicted_labels) # Cálculo f1 score\n",
    "\n",
    "    return confusion_mat, precision, accuracy, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af37b93",
   "metadata": {},
   "source": [
    "### APLICACIÓN DEL MÉTODO EN LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_scores = []\n",
    "for key in loaded_data.files:\n",
    "    signal_dict = loaded_data[key].item()\n",
    "    for sub_key, signal_array in signal_dict.items():\n",
    "        AverPSD, k_score, Valt = SM_TWA(signal_array) # Aplicamos método espectral\n",
    "        K_scores.append(k_score) # se obtienen los k-score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9bfbb2",
   "metadata": {},
   "source": [
    "### EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = assign_labels(K_scores)\n",
    "# contar cuantos segmentos tienen alternancia y cuántos no\n",
    "count_0 = labels.count(0)\n",
    "count_1 = labels.count(1)\n",
    "\n",
    "print(\"Number of 0 labels:\", count_0) \n",
    "print(\"Number of 1 labels:\", count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las etiquetas verdaderas de la base de datos, sabiendo que la mitad son con alternancia y la otra mitad si alternancia\n",
    "\n",
    "true_labels = np.concatenate([np.ones((total_length) // 2), np.zeros((total_length) // 2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos las métricas\n",
    "\n",
    "confusion_mat, precision, accuracy, recall, f1 = evaluate_performance(true_labels, labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el gráfico\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['No Alternancia', 'Alternancia '], yticklabels=['No Alternancia', 'Alternancia '])\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión del Método Espectral (70 µV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15191ad9",
   "metadata": {},
   "source": [
    "## MÉTODO TEMPORAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6350e31",
   "metadata": {},
   "source": [
    "### FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071639bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def TimeMethod_TWA(M_Wind):\n",
    "\"\"\"\n",
    "Esta función aplica el Método del Tiempo para detectar TWA, tal como se informa en:\n",
    "\n",
    "MG Fernández-Calvillo et al., \"Machine Learning approach for TW detection relying on ensemble data design,\" Heliyon, Vol. 9, No. 1, enero 2023.\n",
    "\n",
    "Parámetros:\n",
    "    M_Wind (array): Array que contiene la señal correspondiente a una ventana de complejos ST-ST.\n",
    "\n",
    "Retornos:\n",
    "    Valt_est (float): Voltaje TWA estimado (en microV).\n",
    "    Valt_wave_est (array): Estimación de ML de la onda alternante (en microV).\n",
    "\"\"\"\n",
    "    M_Wind = np.array(M_Wind)  # Convierte la lista en NumPy array \n",
    "\n",
    "    Valt_wave_est = np.mean(M_Wind[::2, :], axis=0) - np.mean(M_Wind[1::2, :], axis=0)\n",
    "    Valt_est = np.max(np.abs(Valt_wave_est)) * 0.5 * 1e3\n",
    "\n",
    "    return Valt_est, Valt_wave_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c33322",
   "metadata": {},
   "source": [
    "### APLICACIÓN DEL MÉTODO EN LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valt_TM_dict=[]\n",
    "for key in loaded_data.files:\n",
    "    signal_dict = loaded_data[key].item()\n",
    "    for sub_key, signal_array in signal_dict.items():\n",
    "\n",
    "        Valt_TM_est, _ = TimeMethod_TWA(signal_array)\n",
    "        Valt_TM_dict.append(Valt_TM_est)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6829e80",
   "metadata": {},
   "source": [
    "### EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cáculo de la media y la desviación estándar para el subconjunto con alternancia\n",
    "mean_1st_half = np.mean(Valt_TM_dict[:len(Valt_TM_dict)//2 ])\n",
    "std_1st_half = np.std(Valt_TM_dict[:len(Valt_TM_dict)//2 ])\n",
    "\n",
    "# Cáculo de la media y la desviación estándar para el subconjunto sin alternancia\n",
    "mean_2nd_half = np.mean(Valt_TM_dict[len(Valt_TM_dict)//2:])\n",
    "std_2nd_half = np.std(Valt_TM_dict[len(Valt_TM_dict)//2 :])\n",
    "\n",
    "print(\"Primera Mitad:\")\n",
    "print(\"Amplitud Media:\", mean_1st_half)\n",
    "print(\"Deviación Estándar:\", std_1st_half)\n",
    "\n",
    "print(\"Segunda Mitad:\")\n",
    "print(\"Amplitud Media:\", mean_2nd_half)\n",
    "print(\"Deviación Estándar:\", std_2nd_half)\n",
    "\n",
    "# Comparamos la media y la desviación\n",
    "if mean_1st_half > mean_2nd_half:\n",
    "    print(\"La amplitud media es mayor en la primera mitad:\", mean_1st_half )\n",
    "else:\n",
    "    print(\"La amplitud media es mayor en la segunda mitad\", mean_2nd_half)\n",
    "\n",
    "if std_1st_half > std_2nd_half:\n",
    "    print(\"La desviación estándar es mayor en la primera mitad:\", std_1st_half)\n",
    "else:\n",
    "    print(\"La desviación estándar es mayor en la segunda mitad:\", std_2nd_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15504e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_with_alternans = Valt_TM_dict[:len(Valt_TM_dict)//2]\n",
    "group_without_alternans = Valt_TM_dict[len(Valt_TM_dict)//2:]\n",
    "\n",
    "# Creamos una lista con los dos subconjuntos\n",
    "data = [group_with_alternans, group_without_alternans]\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Creamos el boxplot\n",
    "boxplot = ax.boxplot(data, patch_artist=True, notch=True)\n",
    "\n",
    "# Establecer los colores del boxplot\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for patch, color in zip(boxplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Agregar etiquetas y título\n",
    "ax.set_xticklabels(['Grupo con Alternancias', 'Grupo sin Alternancias'])\n",
    "ax.set_ylabel('Amplitud (µV)')\n",
    "plt.title('Distribución de la Amplitud a 70µV')\n",
    "plt.axhline(70) #umbral dependiendo del voltaje de amplitud\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd1fa2",
   "metadata": {},
   "source": [
    "## MÉTODO  DE LA RAZÓN DE PROBABILIDAD LAPLACIANA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43370c86",
   "metadata": {},
   "source": [
    "### FUNCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c37d4",
   "metadata": {},
   "source": [
    "A) FUNCIÓN MÉTODO DE LA RAZÓN DE PROBABILIDAD LAPLACIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLR_method(M_Wind, D):\n",
    "# Esta función aplica el método de la Razón de Verosimilitud Laplaciana para detectar TWA.\n",
    "# Reportado en: Juan Pablo Martínez et al., \"Characterization of repolarization alternans during ischemia: Time-Course and spatial analysis,\" IEEE Trans Bio Eng, Vol. 53, No. 4, abril 2006.\n",
    "#\n",
    "# Parámetros de entrada:\n",
    "#   - M_Wind: array que contiene la señal correspondiente a una ventana.\n",
    "#   - D: tasa de reducción de datos, se propone establecerla en 8.\n",
    "# Parámetros de salida:\n",
    "#   - Valt_est: voltaje TWA estimado (en microV).\n",
    "#   - Valt_wave_est: estimación de ML de la onda alternante (en microV).\n",
    "#   - T: parámetro de significancia estadística.\n",
    "\n",
    "    X_decimated = resample(M_Wind.T, int(len(M_Wind.T)/D)).T # Aplicamos un diezmado \n",
    "    M = M_Wind.shape[0]  # Dimensión de latido \n",
    "    P = X_decimated.shape[1]\n",
    "    Alt_matrix = np.tile(((-np.ones(M))**(np.arange(M))),(P,1)).T\n",
    "    CompDem_X = X_decimated* Alt_matrix\n",
    "    Valt_est_wave = np.median(CompDem_X, axis=0)\n",
    "    Valt_est = np.sqrt(np.mean(Valt_est_wave**2))* 0.5 * 1e3\n",
    "    T_series = np.zeros(Valt_est_wave.size)\n",
    "\n",
    "    for p in range(len(T_series)):\n",
    "        aux = CompDem_X[:, p]\n",
    "        if Valt_est_wave[p] < 0:\n",
    "            T_series[p] = 2 * np.sum(np.abs(aux[(aux < 0) & (aux > Valt_est_wave[p])]))\n",
    "        else:\n",
    "            T_series[p] = 2 * np.sum(abs(aux[(aux > 0) & (aux < Valt_est_wave[p])]))\n",
    "                  \n",
    "    MLE_noise = np.sum(np.sum(np.abs(CompDem_X - Valt_est_wave)))\n",
    "    T = np.sum(T_series) / MLE_noise\n",
    "\n",
    "    \n",
    "    return Valt_est, Valt_est_wave, T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d224c",
   "metadata": {},
   "source": [
    "B) FUNCIÓN DE CLASIFICIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels_LLR(T):\n",
    "    labels_LLR = [] # Lista para almacenar los 0 o 1\n",
    "\n",
    "    for T in T_series:\n",
    "        if T < 0.15: # el umbral se establece en 0.15\n",
    "            labels_LLR.append(0)# no posee alternancia, se agrega a la lista como 0 porque el T es menor que 0.15\n",
    "        else:\n",
    "            labels_LLR.append(1) # posee alternancia, se agrega a la lista como 1 porque T es mayor que 0.15\n",
    "\n",
    "    return labels_LLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fb3ec",
   "metadata": {},
   "source": [
    "### APLICACIÓN DEL MÉTODO EN LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valt_est_dict=[]\n",
    "T_series=[]\n",
    "for key in loaded_data.files:\n",
    "    signal_dict = loaded_data[key].item()\n",
    "    \n",
    "    for sub_key, signal_array in signal_dict.items():\n",
    "        Valt_est,  Valt_est_wave, T = LLR_method(signal_array,1)\n",
    "        T_series.append(T)\n",
    "        Valt_est_dict.append(Valt_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3f50c",
   "metadata": {},
   "source": [
    "### EVALUACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dda5a7",
   "metadata": {},
   "source": [
    "a) Estimación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_with_alternans = Valt_est_dict[:len(Valt_est_dict)//2]\n",
    "group_without_alternans = Valt_est_dict[len(Valt_est_dict)//2:]\n",
    "\n",
    "# Creamos una lista con los dos subconjuntos\n",
    "data = [group_with_alternans, group_without_alternans]\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = ax.boxplot(data, patch_artist=True, notch=True)\n",
    "\n",
    "# Creamos el boxplot\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for patch, color in zip(boxplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Establecer los colores del boxplot\n",
    "ax.set_xticklabels(['Grupo con Alternancias', 'Grupo sin Alternancias'])\n",
    "ax.set_ylabel('Amplitud (µV) ')\n",
    "plt.title('Distribución de la Amplitud a 70µV')\n",
    "plt.axhline(70)  #umbral dependiendo del voltaje de amplitud\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáculo de la media y la desviación estándar para el subconjunto con alternancia\n",
    "mean_1st_half = np.mean(Valt_est_dict[:len(Valt_est_dict)//2 ])\n",
    "std_1st_half = np.std(Valt_est_dict[:len(Valt_est_dict)//2 ])\n",
    "\n",
    "# Cáculo de la media y la desviación estándar para el subconjunto sin alternancia\n",
    "mean_2nd_half = np.mean(Valt_est_dict[len(Valt_est_dict)//2:])\n",
    "std_2nd_half = np.std(Valt_est_dict[len(Valt_est_dict)//2 :])\n",
    "\n",
    "print(\"Primera Mitad:\")\n",
    "print(\"Amplitud Media:\", mean_1st_half)\n",
    "print(\"Deviación Estándar:\", std_1st_half)\n",
    "\n",
    "print(\"Segunda Mitad:\")\n",
    "print(\"Amplitud Media:\", mean_2nd_half)\n",
    "print(\"Deviación Estándar:\", std_2nd_half)\n",
    "\n",
    "# Comparamos la media y la desviación\n",
    "if mean_1st_half > mean_2nd_half:\n",
    "    print(\"La amplitud media es mayor en la primera mitad:\", mean_1st_half )\n",
    "else:\n",
    "    print(\"La amplitud media es mayor en la segunda mitad\", mean_2nd_half)\n",
    "\n",
    "if std_1st_half > std_2nd_half:\n",
    "    print(\"La desviación estándar es mayor en la primera mitad:\", std_1st_half)\n",
    "else:\n",
    "    print(\"La desviación estándar es mayor en la segunda mitad:\", std_2nd_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be40c54",
   "metadata": {},
   "source": [
    "b) Detección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_LLR = assign_labels_LLR(T_series)\n",
    "# contar cuantos segmentos tienen alternancia y cuántos no\n",
    "count_0_LLR = labels_LLR.count(0)\n",
    "count_1_LLR = labels_LLR.count(1)\n",
    "\n",
    "print(\"Number of 0 labels:\", count_0_LLR)\n",
    "print(\"Number of 1 labels:\", count_1_LLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos métricas\n",
    "\n",
    "confusion_mat, precision, accuracy, recall, f1 = evaluate_performance(true_labels, labels_LLR)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el gráfico\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Reds', xticklabels=['No Alternancia', 'Alternancia '], yticklabels=['No Alternancia', 'Alternancia '])\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión del Método de la Razón de Probabilidad Laplaciana(70 µV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b4e6e4",
   "metadata": {},
   "source": [
    "## MÉTODO DE PROMEDIO MÓVIL MODIFICADO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ba8e9",
   "metadata": {},
   "source": [
    "### FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMA_TWA(x, First_2_ST_estimates, WindStep):\n",
    "    # Inicializamos las variables\n",
    "    M, N = x.shape\n",
    "    x_est = np.zeros((M + 2, N))\n",
    "    x_est[0:2, :] = First_2_ST_estimates[0:2, :]\n",
    "    Fraction_error_estimate = 8\n",
    "\n",
    "    # Estimamos el segmento ST\n",
    "    for k1 in range(2, M + 2):\n",
    "        e = (x[k1 - 2, :] - x_est[k1 - 2, :]) / Fraction_error_estimate\n",
    "        h = np.zeros(N)\n",
    "        for k2 in range(N):\n",
    "            if e[k2] <= -0.032:\n",
    "                h[k2] = -0.032\n",
    "            elif -0.032 < e[k2] <= -0.001:\n",
    "                h[k2] = e[k2]\n",
    "            elif -0.001 < e[k2] < 0:\n",
    "                h[k2] = -0.001\n",
    "            elif e[k2] == 0:\n",
    "                h[k2] = 0\n",
    "            elif 0 < e[k2] <= 0.001:\n",
    "                h[k2] = 0.001\n",
    "            elif 0.001 < e[k2] <= 0.032:\n",
    "                h[k2] = e[k2]\n",
    "            else:\n",
    "                h[k2] = 0.032\n",
    "        x_est[k1, :] = x_est[k1 - 2, :] + h\n",
    "\n",
    "    # Calculamos las salidas\n",
    "    Next_2_ST_estimates = x_est[WindStep:WindStep + 2, :]\n",
    "    x_est = x_est[:M, :]\n",
    "\n",
    "    aux = np.diff(x_est, axis=0)  # Determinar la primera diferencia por filas\n",
    "    V_alt_MMA = aux[::2, :] * 1e3  # En microvoltios\n",
    "    V_MMA = np.max(np.abs(V_alt_MMA), axis=1)\n",
    "\n",
    "    return V_MMA, V_alt_MMA, Next_2_ST_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f753f",
   "metadata": {},
   "source": [
    "### APLICACIÓN DEL MÉTODO EN LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WindStep= 8\n",
    "MethPred_Predictor_dict=[]\n",
    "\n",
    "for key in loaded_data.files:\n",
    "    signal_dict = loaded_data[key].item()\n",
    "    for sub_key, signal_array in signal_dict.items():\n",
    "        x_est_input = np.zeros((2, signal_array.shape[1]))\n",
    "        x = signal_array\n",
    "        V, _, x_est_out = MMA_TWA(x, x_est_input, WindStep)\n",
    "        MethPred_Predictor_dict.append(V[-1])\n",
    "        x_est_input = x_est_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cedaff",
   "metadata": {},
   "source": [
    "### EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609aa5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáculo de la media y la desviación estándar para el subconjunto con alternancia\n",
    "mean_1st_half = np.mean(MethPred_Predictor_dict[:len(MethPred_Predictor_dict)//2 ])\n",
    "std_1st_half = np.std(MethPred_Predictor_dict[:len(MethPred_Predictor_dict)//2 ])\n",
    "\n",
    "# Cáculo de la media y la desviación estándar para el subconjunto sin alternancia\n",
    "mean_2nd_half = np.mean(MethPred_Predictor_dict[len(MethPred_Predictor_dict)//2:])\n",
    "std_2nd_half = np.std(MethPred_Predictor_dict[len(MethPred_Predictor_dict)//2 :])\n",
    "\n",
    "print(\"Primera Mitad:\")\n",
    "print(\"Amplitud Media:\", mean_1st_half)\n",
    "print(\"Deviación Estándar:\", std_1st_half)\n",
    "\n",
    "print(\"Segunda Mitad:\")\n",
    "print(\"Amplitud Media:\", mean_2nd_half)\n",
    "print(\"Deviación Estándar:\", std_2nd_half)\n",
    "\n",
    "# Comparamos la media y la desviación\n",
    "if mean_1st_half > mean_2nd_half:\n",
    "    print(\"La amplitud media es mayor en la primera mitad:\", mean_1st_half )\n",
    "else:\n",
    "    print(\"La amplitud media es mayor en la segunda mitad\", mean_2nd_half)\n",
    "\n",
    "if std_1st_half > std_2nd_half:\n",
    "    print(\"La desviación estándar es mayor en la primera mitad:\", std_1st_half)\n",
    "else:\n",
    "    print(\"La desviación estándar es mayor en la segunda mitad:\", std_2nd_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_with_alternans = MethPred_Predictor_dict[:len(MethPred_Predictor_dict)//2]\n",
    "group_without_alternans = MethPred_Predictor_dict[len(MethPred_Predictor_dict)//2:]\n",
    "\n",
    "# Creamos una lista con los dos subconjuntos\n",
    "data = [group_with_alternans, group_without_alternans]\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Crear un boxplot\n",
    "boxplot = ax.boxplot(data, patch_artist=True, notch=True)\n",
    "\n",
    "# Establecer los colores del boxplot\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for patch, color in zip(boxplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Añadimos el título y los ejes de coordenadas\n",
    "ax.set_xticklabels(['Grupo con Alternancias', 'Grupo sin Alternancias'])\n",
    "ax.set_ylabel('Amplitud µV')\n",
    "plt.title('Distribución de la Amplitud a 70µV')\n",
    "plt.axhline(70)#establecemos el umbral dependiendo del voltaje de amplitud\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3300b",
   "metadata": {},
   "source": [
    "## MÉTODO DE CORRELACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9a1bb",
   "metadata": {},
   "source": [
    "A) FUNCIÓN MÉTODO DE CORRELACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlation_Method_TWA(M_matrix):\n",
    "    Tmdn = np.median(M_matrix, axis=0)  # Mediana de la onda T: en toda la ventana\n",
    "    ACI = np.sum(M_matrix * Tmdn, axis=1) / np.sum(Tmdn ** 2)  # Índice de correlación de alternancia (ACI)\n",
    "    ACI_1_wind = ACI - 1\n",
    "    Acm_wind = 2 * (np.abs(ACI_1_wind)) * ((np.sum(Tmdn ** 2)) / (np.sum(np.abs(Tmdn))))  # Estimación de la amplitud de TWA\n",
    "\n",
    "    # Función para calcular la tasa de cruces por cero\n",
    "    def zerocrossrate(signal):\n",
    "        crossings = [0] * len(signal)\n",
    "        for i in range(1, len(signal)):\n",
    "            if (signal[i - 1] >= 0 and signal[i] < 0) or (signal[i - 1] < 0 and signal[i] >= 0):\n",
    "                crossings[i] = 1\n",
    "        return crossings\n",
    "\n",
    "    indices = zerocrossrate(ACI_1_wind)\n",
    "    indices_restados = np.diff(np.array([0] + list(indices) + [0]))\n",
    "    f = np.where((indices_restados == 1) | (indices_restados == -1))[0]\n",
    "    consec_count = f[1::2] - f[::2]\n",
    "\n",
    "    if consec_count.size > 0:\n",
    "        zcr_max_wind = np.max(consec_count)\n",
    "    else:\n",
    "        zcr_max_wind = 0  # O cualquier otro valor predeterminado que tenga sentido en tu contexto\n",
    "\n",
    "    return zcr_max_wind, Acm_wind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd662f49",
   "metadata": {},
   "source": [
    "B) FUNCIÓN DE CLASIFICACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdac738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels_CM(zcr_max):\n",
    "    labels_CM = [] # Lista para almacenar los 0 o 1\n",
    "\n",
    "    for zcr_max in zcr_max_dict:\n",
    "        if zcr_max < 5:  # el umbral se establece en 5\n",
    "            labels_CM.append(0)# no posee alternancia, se agrega a la lista como 0 porque el zc_max es menor que 5\n",
    "        else:\n",
    "            labels_CM.append(1)# posee alternancia, se agrega a la lista como 1 porque el zc_max es mayor que 5\n",
    "\n",
    "    return labels_CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3553d2",
   "metadata": {},
   "source": [
    "### APLICACIÓN DEL MÉTODO EN LA BASE DE DATOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_max_dict = [] \n",
    "\n",
    "for key in loaded_data.files:\n",
    "    signal_dict = loaded_data[key].item()\n",
    "    for sub_key, signal_array in signal_dict.items():\n",
    "        zcr_max,_ = Correlation_Method_TWA(signal_array)\n",
    "        zcr_max_dict.append(zcr_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6f554",
   "metadata": {},
   "source": [
    "### EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48690161",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_CM = assign_labels_CM(zcr_max)\n",
    "# contar cuantos segmentos tienen alternancia y cuántos no\n",
    "count_0_CM = labels_CM.count(0)\n",
    "count_1_CM = labels_CM.count(1)\n",
    "\n",
    "print(\"Number of 0 labels:\", count_0_CM)\n",
    "print(\"Number of 1 labels:\", count_1_CM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f794a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos métricas\n",
    "\n",
    "confusion_mat, precision, accuracy, recall, f1 = evaluate_performance(true_labels, labels_CM)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9aba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el gráfico\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Greens',xticklabels=['No Alternancia', 'Alternancia '], yticklabels=['No Alternancia', 'Alternancia '])\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión del Método de Correlación (70 µV)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
